{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to iRec documentation!","text":"<p>This is a specialized library containing Multi-Armed Bandit, Active Learning and others methods. A full environment to code yours Reinforcement Learning Recommender Systems. Our goal is to encourage the evaluation of reproducible offline experiments by providing simple building blocks for running robust experiments and an extremely intuitive platform. Our framework can be used to share environments reference RSs and reusable implementations of reference RS agents. Thus, we built a complete structure, from data entry and manipulation to the evaluation of the results obtained, using several evaluation metrics that are perfectly capable of measuring the quality of the recommendation.</p> <p>Unlike existing frameworks, our structure has the most recent and relevant RL algorithms published so far, in addition to providing different forms and evaluation metrics, generalizable in different situations and ideal for the scenario of recommendation systems.</p> <p></p>"},{"location":"#main-features","title":"Main features:","text":"<ul> <li>Several state-of-the-art reinforcement learning models for the recommendation scenario</li> <li>Novelty, coverage and much more different type of online metrics</li> <li>Integration with the most used datasets for evaluating recommendation systems</li> <li>Flexible configuration</li> <li>Modular and reusable design</li> <li>Contains multiple evaluation policies currently used in the literature to evaluate reinforcement learning models</li> <li>Online Learning and Reinforcement Learning models</li> <li>Metrics and metrics evaluators are awesome to evaluate recommender systems in different ways</li> </ul> <p>Also, we provide a amazing application created using the iRec library (under the app/ folder) that can be used to setup a experiment under 5~ minutes with parallel processes, log registry and results views. The main features are:</p> <ul> <li>Powerful application to run any reinforcement learning experiment powered by MLflow</li> <li>Entire pipeline of execution is fully parallelized</li> <li>Log registry</li> <li>Results views</li> <li>Statistical test</li> <li>Extensible environment</li> </ul>"},{"location":"#get-started","title":"GET STARTED","text":"<ul> <li>Introduction</li> <li>Install iRec</li> <li>Quick Start</li> <li>Release Notes</li> </ul>"},{"location":"#running-experiments","title":"RUNNING EXPERIMENTS","text":"<ul> <li>Configurations Files</li> <li>Running an Experiment</li> </ul>"},{"location":"#modules","title":"MODULES","text":"<ul> <li>Models</li> <li>Evaluation Policies</li> <li>Metric Evaluators</li> <li>Metrics</li> </ul>"},{"location":"guide/install_irec/","title":"Installation","text":"<p>iRec works with the following operating systems:</p> <ul> <li> <p>Linux</p> </li> <li> <p>Windows 10</p> </li> <li> <p>macOS X</p> </li> </ul> <p>The library currently supports Python \u2265 3.8. There are several ways to install iRec:</p>"},{"location":"guide/install_irec/#from-pypi","title":"From PyPI:","text":"<pre><code>pip install irec\n</code></pre>"},{"location":"guide/install_irec/#from-the-github-source-for-latest-updates","title":"From the GitHub source (for latest updates):","text":"<pre><code>git clone https://github.com/irec-org/irec.git\npip install -e irec\n</code></pre>"},{"location":"guide/introduction/","title":"Introduction","text":"<p>This is a specialized library containing Multi-Armed Bandit, Active Learning and others methods. A full environment to code yours Reinforcement Learning Recommender Systems. Our goal is to encourage the evaluation of reproducible offline experiments by providing simple building blocks for running robust experiments and an extremely intuitive platform. Our framework can be used to share environments, baseline recommendation systems (RSs) and reusable implementations of baseline RS agents. Thus, we built a complete structure, from data entry and manipulation to the evaluation of the results obtained, using several evaluation metrics that are perfectly capable of measuring the quality of the recommendation.</p> <p>Unlike existing frameworks, our structure has the most recent and relevant RL algorithms published so far, in addition to providing different forms and evaluation metrics, generalizable in different situations and ideal for the scenario of recommendation systems.</p> <p></p>"},{"location":"guide/introduction/#main-features","title":"Main features:","text":"<ul> <li>Several state-of-the-art reinforcement learning models for the recommendation scenario</li> <li>Novelty, coverage and much more different type of online metrics</li> <li>Integration with the most used datasets for evaluating recommendation systems</li> <li>Flexible configuration</li> <li>Modular and reusable design</li> <li>Contains multiple evaluation policies currently used in the literature to evaluate reinforcement learning models</li> <li>Online Learning and Reinforcement Learning models</li> <li>Metrics and metrics evaluators are awesome to evaluate recommender systems in different ways</li> </ul> <p>Also, we provide a amazing application created using the iRec library (under the app/ folder) that can be used to setup a experiment under 5~ minutes with parallel processes, log registry and results views. The main features are:</p> <ul> <li>Powerful application to run any reinforcement learning experiment powered by MLflow</li> <li>Entire pipeline of execution is fully parallelized</li> <li>Log registry</li> <li>Results views</li> <li>Statistical test</li> <li>Extensible environment</li> </ul>"},{"location":"guide/quick_start/","title":"Quick Start","text":"<p>Under app/ folder is a example of a application using irec and mlflow, where different experiments can be run with easy using existing recommender systems.</p>"},{"location":"guide/quick_start/#example","title":"Example","text":"<p>Check this example of a execution using the example application:</p> <pre><code>dataset=(\"Netflix 10k\" \"Good Books\" \"Yahoo Music 10k\");\\\nmodels=(Random MostPopular UCB ThompsonSampling EGreedy);\\\nmetrics=(Hits Precision Recall);\\\neval_pol=(\"FixedInteraction\");\nmetric_evaluator=\"Interaction\";\\\n\ncd agents &amp;&amp;\npython run_agent_best.py --agents \"${models[@]}\" --dataset_loaders \"${dataset[@]}\" --evaluation_policy \"${eval_pol[@]}\" &amp;&amp;\n\ncd ../evaluation &amp;&amp;\npython eval_agent_best.py --agents \"${models[@]}\" --dataset_loaders \"${dataset[@]}\" --evaluation_policy \"${eval_pol[@]}\" --metrics \"${metrics[@]}\" --metric_evaluator \"${metric_eval[@]}\" &amp;&amp;\n\npython print_latex_table_results.py --agents \"${models[@]}\" --dataset_loaders \"${dataset[@]}\" --evaluation_policy \"${eval_pol[@]}\" --metric_evaluator \"${metric_eval[@]}\" --metrics \"${metrics[@]}\"\n</code></pre> <p>For more details, please take a look at our tutorials</p>"},{"location":"modules/evaluation_policies/","title":"Evaluation Policies","text":"<p>This module is responsible for determining how the recommendation agent will interact with the previously defined environment. Basically, it implements the classical rein forcement learning algorithm where the system:</p> <p>(1) selects the target user;</p> <p>(2) gets the action from the recommendation model;</p> <p>(3) receives the feedback from the user to that specific action;</p> <p>(4) updates the model\u2019s knowledge with this reward</p> <p>The Evaluation Policies supported by iRec are listed below.</p> Evaluation Policy The base class for all evaluation policies. FixedInteraction Each user is randomly selected and each action will not be per- formed more than once for him/her. Each user will be selected for \ud835\udc47 times. Thus, the system will perform \ud835\udc47 \u00d7 |\ud835\udc48 | iterations, where |\ud835\udc48 | is the number of distinct users available for the evaluation. The number \ud835\udc47 is predefined by the researcher as a parameter. Limited Interaction The system will perform new actions until it hits all items registered in the user historical. Theidea is to make an exhaustive experiment to observe which algo- rithm takes more time to reach all items previous rated by each user."},{"location":"modules/metric_evaluators/","title":"Metric Evaluators","text":"<p>This module aims to guide the entire evalua- tion process over the logs from each iteration of the Evaluation Policy. As the iRec stores each execution log, the researcher can define how s/he would like to evaluate the actions selected by the recommendation model after all interactions.</p> <p>The Metric Evaluators supported by iRec are listed below.</p> Metric Evaluator Description Interaction it evaluates the selected metrics\u2019 overall interactions registered during the recommendation process. Given a scenario in which 100 interactions were performed, for instance, this strategy would evaluate each one separately. Stage Iterations it first aggregates some consecutive interactions in an interval to then evaluate the selected metrics over each group. For instance, in an execution of 10 interactions, the researcher can define two intervals to be evaluated by the system. Total it evaluates the whole recommendation process as one unique procedure. For example, if certain items were recommended during 100 interactions, the metric will be calculated only at the 100th interaction. User Cumulative Interaction it evaluates the interactions cumulatively from the first one until a specific value. In this way, the researcher can evaluate the accumulated result from the 1st to the 10th interaction, then from the 1st to the 15th interaction, and so on."},{"location":"modules/metrics/","title":"Metrics","text":"<p>This module contains all evaluation metrics available in the iRec. Its goal is to provide distinct options to be selected during the previous setup. These metrics are suitable to the recommendation scenario and are usually split into a few groups: Accuracy Coverage, Novelty and Diversity. In our architecture, they follow an implementation pattern where each metric has two methods: </p> <p>(1) compute, in which the entire calculation is performed for a given user;</p> <p>(2) update, which updates the historic of items in each user during the interactive scenario. </p> <p>The recommender metrics supported by iRec are listed below.</p> Metric Reference Description Hits Link Number of recommendations made successfully. Precision Link Precision is defined as the percentage of predictions we get right. Recall Link Represents the probability that a relevant item will be selected. EPC Link Represents the novelty for each user and it is measured by the expected number of seen relevant recommended items not previously seen. EPD Link EPD is a distance-based novelty measure, which looks at distances between the items in the user\u2019s profile and the recommended items. ILD Link It represents the diversity between the list of items recommended. This diversity is measured by the Pearson correlation of the item\u2019s features vector. Gini Coefficient Link Diversity is represented as the Gini coefficient \u2013 a measure of distributional inequality. It is measured as the inverse of cumulative frequency that each item is recommended. Users Coverage Link It represents the percentage of distinct users that are interested in at least k items recommended (k \u2265 1)."},{"location":"modules/models/","title":"Recommendation Agents","text":"<p>This component aims to define an agent to interact with the previously defined environment by making recommendations and receiving the user\u2019s feedback. Similar to the approaches used in the Reinforcement Learning theory, an agent is represented by two main components: a Value Function and an Action Selection Policy. The value function represents the agent\u2019s goals, quantifying the expected consequences of its decisions. In this case, where the agent is a recommendation system, the value function represents the utility of each item for a user according to the algorithm prediction. The reward usually consists of a scalar value representative of the user\u2019s explicit/implicit feedback. In turn, the action selection policy represents the policy used by the agent to choose one (or more) items to be recommended. </p> <p>The Recommendation Agents supported by iRec are listed below.</p> Year Model Paper Description 2002 \u03b5-Greedy Link In general, \u03b5-Greedy models the problem based on an \u03b5 diversification parameter to perform random actions. 2013 Linear \u03b5-Greedy Link A linear exploitation of the items latent factors defined by a PMF formulation that also explore random items with probability \u03b5. 2011 Thompson Sampling Link A basic item-oriented bandit algorithm that follows a Gaussian distribution of items and users to perform the prediction rule based on their samples. 2013 GLM-UCB Link It follows a similar process as Linear UCB based on the PMF formulation, but it also adds a sigmoid form in the exploitation step and makes a time-dependent exploration. 2018 ICTR Link It is an interactive collaborative topic regression model that utilizes the TS bandit algorithm and controls the items dependency by a particle learning strategy. 2015 PTS Link It is a PMF formulation for the original TS based on a Bayesian inference around the items. This method also applies particle filtering to guide the exploration of items over time. 2019 kNN Bandit Link A simple multi-armed bandit elaboration of neighbor-based collaborative filtering. A variant of the nearest-neighbors scheme, but endowed with a controlled stochastic exploration capability of the users\u2019 neighborhood, by a parameter-free application of Thompson sampling. 2017 Linear TS Link An adaptation of the original Thompson Sampling to measure the latent dimensions by a PMF formulation. 2013 Linear UCB Link An adaptation of the original LinUCB (Lihong Li et al. 2010) to measure the latent dimensions by a PMF formulation. 2020 NICF Link It is an interactive method based on a combination of neural networks and  collaborative filtering that also performs a meta-learning of the user\u2019s preferences. 2016 COFIBA Link This method relies on upper-confidence-based tradeoffs between exploration and exploitation, combined with adaptive clustering procedures at both the user and the item sides. 2002 UCB Link It is the original UCB that calculates a confidence interval for each item at each iteration and tries to shrink the confidence bounds. 2021 Cluster-Bandit (CB) Link it is a new bandit algorithm based on clusters to face the cold-start problem. 2002 Entropy Link The entropy of an item i is calculated using the relative frequency of the possible ratings. In general, since entropy measures the spread of ratings for an item, this strategy tends to promote rarely rated items, which can be considerably informative. 2002 log(pop)*ent Link It combines popularity and entropy to identify potentially relevant items that also have the ability to add more knowledge to the system. As these concepts are not strongly correlated, it is possible to achieve this combination through a linear combination of the popularity \u03c1 of an item i by its entropy \u03b5: score(i) = log(\u03c1i) \u00b7 \u03b5i. - Random Link This method recommends totally random items. - Most Popular Link It recommends items with the higher number of ratings received (most-popular) at each iteration. - Best Rated Link Recommends top-rated items based on their average ratings in each iteration."},{"location":"run_exp/configuration_files/","title":"Configuration Files","text":"<p>iRec has some configuration files to define an experiment, such as dataset settings, agents, policies and evaluation metrics. Below we present brief examples about each of the files available in this framework.</p> <p>For more details on configuration files, go to configuration_files</p> <p>dataset_loaders.yaml</p> <p>This configuration file stores all the configurations related to the bases that will be used during the execution of an experiment.</p> <pre><code>'MovieLens 100k': # Dataset name\n    FullData: # Loader method\n        dataset: # Info dataset\n          path: ./data/datasets/MovieLens 100k/ratings.csv\n          random_seed: 0\n          file_delimiter: \",\"\n          skip_head: true\n\n        prefiltering: # Filters\n          filter_users: # By Users\n            min_consumption: 50\n            num_users: 100\n          filter_items: # By Items\n            min_ratings: 1\n            num_items: 100\n\n        splitting: # Splitting\n          strategy: temporal\n          train_size: 0.8\n          test_consumes: 5\n\n        validation:\n          validation_size: 0.2\n\ufe19\n</code></pre> <p>dataset_agents.yaml</p> <p>This configuration file stores the settings of the agents (Recommendators) that will be used in the experiments.</p> <pre><code>'MovieLens 100k':\n  LinearUCB:\n    SimpleAgent:\n      action_selection_policy:\n        ASPGreedy: {}\n      value_function:\n        LinearUCB:\n          alpha: 1.0\n          item_var: 0.01\n          iterations: 20\n          num_lat: 20\n          stop_criteria: 0.0009\n          user_var: 0.01\n          var: 0.05\n \ufe19\n</code></pre> <p>agents_variables.yaml</p> <p>In this configuration file it is possible to define a search field for the variables of each agent, which will be used during the grid search</p> <pre><code>GridSearch:\n\n  - EGreedy:\n      SimpleAgent:\n        action_selection_policy:\n          ASPEGreedy:\n            epsilon: linspace(0.001, 1, 10)\n        value_function:\n          EGreedy: {}\n\n  - UCB:\n      SimpleAgent:\n        action_selection_policy:\n          ASPGreedy: {}\n        value_function:\n          UCB:\n            c: linspace(0.001, 1, 10)\n</code></pre> <p>evaluation_policies.yaml</p> <p>In this configuration file the evaluation policies are defined. To carry out an experiment, we need to define how the recommendation process will be, the interactions between user and item, and for that we create an evaluation policy in accordance with the objectives of the experiment.</p> <pre><code>FixedInteraction:\n  num_interactions: 100\n  interaction_size: 1\n  save_info: False\n\n\ufe19\n</code></pre> <p>metric_evaluators.yaml</p> <p>This file defines the evaluation metrics for an experiment. This file is responsible for providing details on how to assess the interactions performed during the assessment process.</p> <pre><code>UserCumulativeInteraction:\n  interaction_size: 1\n  interactions_to_evaluate:\n    - 5\n    - 10\n    - 20\n    - 50\n    - 100\n  num_interactions: 100\n  relevance_evaluator_threshold: 3.999\n\n\ufe19\n</code></pre> <p>defaults.yaml</p> <p>This configuration file is a way to define the general settings for an experiment, here we can define the agents, the base, the policy and the evaluation metric, as well as some additional information.</p> <pre><code>agent: LinearUCB\nagent_experiment: agent\ndata_dir: data/\ndataset_experiment: dataset\ndataset_loader: 'MovieLens 1M'\nevaluation_experiment: evaluation\nevaluation_policy: FixedInteraction\nmetric: Hits\nmetric_evaluator: UserCumulativeInteraction\npdf_dir: pdf/\ntex_dir: tex/\n</code></pre> <p>For more details, please take a look at our tutorials</p>"},{"location":"run_exp/run_experiment/","title":"Run Experiment","text":"<p>To see the full configuration file please visit the following: configuration_files</p> <p>To run the experiment use the following script_advanced: run_experiment</p> <p>To see how to use our library: using the library</p>"}]}